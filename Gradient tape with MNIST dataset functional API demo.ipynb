{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcf5c973",
   "metadata": {},
   "source": [
    "# Gradient Tape with Tensorflow - MNIST - Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401539c7",
   "metadata": {},
   "source": [
    "In this notebook, we look at an example of training a Tensorflow model using Gradient Tape\n",
    "To run this notebook, I am using the following package versions:\n",
    "\n",
    "* Tensorflow: 2.8.0\n",
    "\n",
    "* OpenCV: 4.6.0\n",
    "\n",
    "* Tensorflow datasets: 4.6.0\n",
    "\n",
    "There might be minor variations in the code for different versions of these packages.\n",
    "\n",
    "Also this implementation assumes some knowledge of Tensorflow Functional API. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cc6c1a",
   "metadata": {},
   "source": [
    "### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77b791dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.transform import resize \n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import vgg16\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import matplotlib.ticker as mticker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83ca636",
   "metadata": {},
   "source": [
    "### Load, split and understand the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad602b1",
   "metadata": {},
   "source": [
    "Under this section we load the dateset from tensorflow datasets and split the data into **train** and **test**. For more information on how the split is performed check out this link: https://github.com/tensorflow/datasets/blob/master/docs/splits.md\n",
    "\n",
    "Once we have performed the split we want to get a bit more information on how the data is stored after loading. Understanding the datatype beforehand cause prevent a lot of issues from occuring later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35c203a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, info = tfds.load(\"mnist\", split = \"train\", with_info = True)\n",
    "test_data = tfds.load(\"mnist\", split = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3009177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec={'image': TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None)}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b026e35a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='mnist',\n",
       "    full_name='mnist/3.0.1',\n",
       "    description=\"\"\"\n",
       "    The MNIST database of handwritten digits.\n",
       "    \"\"\",\n",
       "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
       "    data_path='~\\\\tensorflow_datasets\\\\mnist\\\\3.0.1',\n",
       "    file_format=tfrecord,\n",
       "    download_size=11.06 MiB,\n",
       "    dataset_size=21.00 MiB,\n",
       "    features=FeaturesDict({\n",
       "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
       "    }),\n",
       "    supervised_keys=('image', 'label'),\n",
       "    disable_shuffling=False,\n",
       "    splits={\n",
       "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
       "        'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
       "    },\n",
       "    citation=\"\"\"@article{lecun2010mnist,\n",
       "      title={MNIST handwritten digit database},\n",
       "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
       "      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n",
       "      volume={2},\n",
       "      year={2010}\n",
       "    }\"\"\",\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80405e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = info.features[\"label\"].num_classes\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c3dd3bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = info.features[\"label\"].names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bba137",
   "metadata": {},
   "source": [
    "### Data formatting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b97a53",
   "metadata": {},
   "source": [
    "We have seen earlier that the images are of size 28*28. Here we format the images such that it can be consumed in the input layer of our model. For the sake of understanding, we will create a dummy tensor with the same shape as the input image, pass it through the formatting function and understand the dimensions of the input to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c8edcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_image(data):\n",
    "    \"\"\"\n",
    "       Function to reshape, format and normalize input images to make it compatible with the deep learning model\n",
    "       Inputs:\n",
    "       data: Input image\n",
    "       Outputs:\n",
    "       image: Formatted image\n",
    "       data[\"label\"]: Training label associated with the image\n",
    "    \"\"\"\n",
    "    image = data[\"image\"]\n",
    "    image = tf.reshape(image, [-1])\n",
    "    image = tf.cast(image, 'float32')\n",
    "    image = image / 255.0\n",
    "    return image, data[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b988334",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_tensor=tf.ones([28, 28,1], tf.int32)*255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc69d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_dict={\"image\":dummy_tensor, \"label\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c307a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_dummy=format_image(dummy_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bd767da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([784])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_dummy[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295bfa58",
   "metadata": {},
   "source": [
    "If you have any issues understanding the different stages of the function you can add print statments after each step. Remember to remove/comment them out before going to the next cell though!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5163df71",
   "metadata": {},
   "source": [
    "Now lets apply the ***format_image*** function on the entire train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40b6fee2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_data = train_data.map(format_image)\n",
    "test_data = test_data.map(format_image)\n",
    "train = train_data.shuffle(buffer_size=1024).batch(batch_size)\n",
    "test =  test_data.batch(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b72b71e",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12049284",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the model,\n",
    "def base_model():\n",
    "    \"\"\"\n",
    "    Define the model architecture here\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=(784,), name='digits')\n",
    "    x = tf.keras.layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "    outputs = tf.keras.layers.Dense(10, activation='softmax', name='predictions')(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53447434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Optimizer and Loss Function\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c34dadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Metrics \n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6052b0d3",
   "metadata": {},
   "source": [
    "### Define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2814933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gradient(optimizer, model, x, y):\n",
    "    \"\"\"\n",
    "    Function for computing gradient and updating the weights\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Get model prediction and compute the loss\n",
    "        logits = model(x)\n",
    "        loss_value = loss_object(y_true=y, y_pred=logits)\n",
    "    # Calculate the gradient using tape.gradient and then update the model weights using our optimizer\n",
    "    gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))  \n",
    "    return logits, loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e99f77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_for_one_epoch():\n",
    "    \"\"\"\n",
    "    Function for computing the gradient and updating weights in each epoch. \n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    # Iterate over batches of the dataset and call apply_gradient function\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train):\n",
    "        logits, loss_value = apply_gradient(optimizer, model, x_batch_train, y_batch_train)     \n",
    "        losses.append(loss_value)\n",
    "        train_acc_metric(y_batch_train, logits)\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb18409b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_validation():\n",
    "    \"\"\"\n",
    "    Function for validation\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    for x_val, y_val in test:\n",
    "        val_logits = model(x_val)\n",
    "        val_loss = loss_object(y_true=y_val, y_pred=val_logits)\n",
    "        losses.append(val_loss)\n",
    "        val_acc_metric(y_val, val_logits)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2948628a",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f52f516",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "\n",
      " Epoch 0: Train loss: 0.3264  Validation Loss: 0.1628, Train Accuracy: 0.9075, Validation Accuracy 0.9517\n",
      "Start of epoch 1\n",
      "\n",
      " Epoch 1: Train loss: 0.1410  Validation Loss: 0.1299, Train Accuracy: 0.9584, Validation Accuracy 0.9620\n",
      "Start of epoch 2\n",
      "\n",
      " Epoch 2: Train loss: 0.1023  Validation Loss: 0.1102, Train Accuracy: 0.9700, Validation Accuracy 0.9665\n",
      "Start of epoch 3\n",
      "\n",
      " Epoch 3: Train loss: 0.0817  Validation Loss: 0.1074, Train Accuracy: 0.9761, Validation Accuracy 0.9665\n",
      "Start of epoch 4\n",
      "\n",
      " Epoch 4: Train loss: 0.0655  Validation Loss: 0.1036, Train Accuracy: 0.9807, Validation Accuracy 0.9680\n",
      "Start of epoch 5\n",
      "\n",
      " Epoch 5: Train loss: 0.0540  Validation Loss: 0.0984, Train Accuracy: 0.9836, Validation Accuracy 0.9716\n",
      "Start of epoch 6\n",
      "\n",
      " Epoch 6: Train loss: 0.0471  Validation Loss: 0.1053, Train Accuracy: 0.9854, Validation Accuracy 0.9693\n",
      "Start of epoch 7\n",
      "\n",
      " Epoch 7: Train loss: 0.0384  Validation Loss: 0.1034, Train Accuracy: 0.9880, Validation Accuracy 0.9701\n",
      "Start of epoch 8\n",
      "\n",
      " Epoch 8: Train loss: 0.0335  Validation Loss: 0.1000, Train Accuracy: 0.9891, Validation Accuracy 0.9729\n",
      "Start of epoch 9\n",
      "\n",
      " Epoch 9: Train loss: 0.0287  Validation Loss: 0.1099, Train Accuracy: 0.9907, Validation Accuracy 0.9702\n"
     ]
    }
   ],
   "source": [
    "model = base_model()\n",
    "\n",
    "# Iterate over epochs\n",
    "epochs = 10\n",
    "epochs_val_losses, epochs_train_losses = [], []\n",
    "for epoch in range(epochs):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "    \n",
    "    #Perform training using gradient tape\n",
    "    losses_train = train_data_for_one_epoch()\n",
    "    train_acc = train_acc_metric.result()\n",
    "    \n",
    "    #Perform validation\n",
    "    losses_val = perform_validation()\n",
    "    val_acc = val_acc_metric.result()\n",
    "    \n",
    "    #Compute mean training and validation loss in the epoch \n",
    "    losses_train_mean = np.mean(losses_train)\n",
    "    losses_val_mean = np.mean(losses_val)\n",
    "    epochs_val_losses.append(losses_val_mean)\n",
    "    epochs_train_losses.append(losses_train_mean)\n",
    "\n",
    "    print('\\n Epoch %s: Train loss: %.4f  Validation Loss: %.4f, Train Accuracy: %.4f, Validation Accuracy %.4f' % (epoch, float(losses_train_mean), float(losses_val_mean), float(train_acc), float(val_acc)))\n",
    "  \n",
    "    train_acc_metric.reset_states()\n",
    "    val_acc_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f88d56a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plots for Evaluation\n",
    "def plot_metrics(train_metric, val_metric, metric_name, title, ylim=5):\n",
    "    plt.title(title)\n",
    "    plt.ylim(0,ylim)\n",
    "    plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(1))\n",
    "    plt.plot(train_metric,color='blue',label=metric_name)\n",
    "    plt.plot(val_metric,color='green',label='val_' + metric_name)\n",
    "\n",
    "plot_metrics(epochs_train_losses, epochs_val_losses, \"Loss\", \"Loss\", ylim=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931842c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "cv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
